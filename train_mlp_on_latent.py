# =============================
# train_mlp_on_latent.py (Phase 2 of STaR-DR)
# =============================
"""
Training of an MLP head on pretrained latent representations (generated by STaR-DR Phase 1).

This script loads pretrained autoencoders (cell and drug) from 
(pretrain_autoencoders.py), then trains a supervised classifier (MLP) on
drug-response pairs.

Key ideas:
- The encoders provide latent vectors z_cell and z_drug.
- The classifier is an MLP trained on the concatenated latent space [z_cell, z_drug].
- To mitigate class imbalance, RandomUnderSampler (RUS) is applied on the training split only.
- Input normalization uses feature-wise L2 norms computed on the raw training split
  (before RUS), and those norms are reused for train (post-RUS) and validation.

Supported regimes:
1) Internal validation:
   - Either a simple 90/10 split (STaRDR_pretrained_training)
   - Or StratifiedKFold cross-validation (cv_train)
2) External test evaluation (is_test=True):
   - Train on source dataset, evaluate on an external dataset (CCLE).
   - Test set is normalized with train-time norms.

Leave-out evaluation :
- Leave-Drug-Out (LDO): exclude selected drugs from training and evaluate only on them.
- Leave-Cell-Out (LCO): exclude selected cell lines from training and evaluate only on them.
This measures generalization to unseen entities.

Outputs:
- Saved trained modules per run_id:
  encoder_cell_trained_run_{run_id}.pt
  encoder_drug_trained_run_{run_id}.pt
  mlp_trained_run_{run_id}.pth
- Saved train-time norms:
  train_cell_l2norms_run_{run_id}.pt
  train_drug_l2norms_run_{run_id}.pt
- Loss/accuracy curves + optional t-SNE plots.
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim import lr_scheduler
from torch.utils.data import DataLoader, TensorDataset
import pandas as pd
import numpy as np
import random
import matplotlib.pyplot as plt
import pickle
from imblearn.under_sampling import RandomUnderSampler
from sklearn.model_selection import train_test_split
from sklearn.manifold import TSNE
from sklearn.model_selection import StratifiedKFold, StratifiedGroupKFold
from data_loader import RawDataLoader
from utils import DATA_MODALITIES, RAW_BOTH_DATA_FOLDER, BOTH_SCREENING_DATA_FOLDER, CCLE_RAW_DATA_FOLDER, CCLE_SCREENING_DATA_FOLDER, TCGA_DATA_FOLDER, TCGA_SCREENING_DATA
from mlp import MLP
from evaluation import Evaluation
from pretrain_autoencoders import SimpleAutoencoder

# Use GPU if available
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(device)

RANDOM_SEED = 42

# Use GPU if available
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(device)

RANDOM_SEED = 42

class STaRDR_pretrained(nn.Module):
    """
    STaR-DR (pretrained) model: two pretrained encoders + an MLP classifier.

    The module wraps:
    - encoder_cell: the encoder part of a pretrained cell autoencoder
    - encoder_drug: the encoder part of a pretrained drug autoencoder
    - mlp: a classifier trained on the concatenated latent vectors

    Parameters:
    - encoder_cell (SimpleAutoencoder): pretrained cell AE.
    - encoder_drug (SimpleAutoencoder): pretrained drug AE.
    - cell_ae_latent_dim (int): dimension of z_cell.
    - drug_ae_latent_dim (int): dimension of z_drug.
    - freeze_encoders (bool): if True, encoder parameters are frozen.

    Forward:
    - Inputs: (cell_x, drug_x)
    - Outputs:
        y_hat (Tensor): predicted probability (after sigmoid) shape (B, 1)
        z_cell (Tensor): latent cell embedding shape (B, cell_latent_dim)
        z_drug (Tensor): latent drug embedding shape (B, drug_latent_dim)
    """

    def __init__(self, encoder_cell, encoder_drug, cell_ae_latent_dim, drug_ae_latent_dim, freeze_encoders=False):
        super(STaRDR_pretrained, self).__init__()

        # Load pretrained encoders
        self.encoder_cell = encoder_cell.encoder
        self.encoder_drug = encoder_drug.encoder

        if freeze_encoders:
            for param in self.encoder_cell.parameters():
                param.requires_grad = False
            for param in self.encoder_drug.parameters():
                param.requires_grad = False
                
        # Initialize MLP
        self.mlp = MLP(cell_ae_latent_dim+drug_ae_latent_dim, 1)


    def forward(self, cell_x, drug_x):
        z_cell = self.encoder_cell(cell_x)
        z_drug = self.encoder_drug(drug_x)

        combined = torch.cat([z_cell, z_drug], dim=1)
        return self.mlp(combined), z_cell, z_drug
        
# -----------------------------
# Global hyperparameters
# -----------------------------

cell_ae_latent_dim = 700
drug_ae_latent_dim = 50
batch_size = 64
num_epochs = 25

# -----------------------------
# Helpers for Leave-Out protocols
# -----------------------------

def parse_cell_ids_from_pairs(index_like):
    """
    Parse cell IDs from an index of pair identifiers formatted as strings.
    - This is used to build group labels for LCO (Leave-Cell-Out) splits.

    Parameters:
    index_like (array-like): Index or list-like object containing pair identifiers (X_pairs.index).

    Returns np.ndarray: Array of extracted cell IDs (dtype=object).
    """
    s = pd.Index(index_like).to_series()
    # Capture everything before the first comma: "(CELL,DRUG)" -> "CELL"
    return s.str.extract(r'^\(([^,]+),')[0].values

def parse_drug_ids_from_pairs(index_like):
    """
    Parse drug IDs from an index of pair identifiers formatted as strings.
    - This is used to build group labels for LDO (Leave-Drug-Out) splits.

    Parameters:
    index_like (array-like): Index or list-like object containing pair identifiers (X_pairs.index).

    Returns np.ndarray: Array of extracted drug IDs (dtype=object).
    """
    s = pd.Index(index_like).to_series()
    # Capture everything after the comma: "(CELL,DRUG)" -> "DRUG"
    return s.str.extract(r'^\([^,]+,\s*([^)]+)\)')[0].values

# -----------------------------
# Training / evaluation functions
# -----------------------------

def STaRDR_pretrained_training(x_cell_train, x_drug_train, y_train, visualize='first', run_id=0):
    """
    Train STaR-DR on a 90/10 split with train-only RUS and train-time normalization.

    Procedure:
    1) Concatenate cell/drug features as pairs (X_pairs).
    2) Stratified split indices into train/val (90/10).
    3) Compute feature-wise L2 norms on the raw train split before RUS
    4) Apply RandomUnderSampler (RUS) to the train split only.
    5) Normalize:
       - train (post-RUS) and val (unresampled) using the same train-time norms.
    6) Load pretrained autoencoders (encoder_cell.pth, encoder_drug.pth),
       build STaRDR_pretrained model, train using train_mlp_with_encoders().
    7) Optionally visualize latent spaces via t-SNE.

    Parameters:
    - x_cell_train (pd.DataFrame): cell features for training pairs.
    - x_drug_train (pd.DataFrame): drug features for training pairs.
    - y_train (array-like): binary labels (0/1).
    - visualize (str): 'never' | 'first' | 'always' for t-SNE plots.
    - run_id (int): used for saving norms and trained weights.

    Return:
    - model (STaRDR_pretrained): trained model on device.
    - cell_norms (torch.Tensor): train-time L2 norms for cell features (CPU).
    - drug_norms (torch.Tensor): train-time L2 norms for drug features (CPU).
    """

    X_pairs = pd.concat([x_cell_train, x_drug_train], axis=1)
    n_cell = x_cell_train.shape[1] 
    y_all   = np.asarray(y_train).ravel()

    # Stratified split of indices
    idx = np.arange(len(y_all))
    train_idx, val_idx = train_test_split(idx, test_size=0.1, random_state=RANDOM_SEED, stratify=y_all)

    # Normalize
    thr = 1e-6
    X_train = X_pairs.iloc[train_idx]
    x_cell_train_tensor = torch.tensor(X_train.iloc[:, :n_cell].values, dtype=torch.float32)
    x_drug_train_tensor = torch.tensor(X_train.iloc[:,  n_cell:].values, dtype=torch.float32)

    cell_norms = torch.norm(x_cell_train_tensor, dim=0, keepdim=True)
    cell_norms = torch.where(cell_norms < thr, torch.ones_like(cell_norms), cell_norms)

    drug_norms = torch.norm(x_drug_train_tensor, dim=0, keepdim=True)
    drug_norms = torch.where(drug_norms < thr, torch.ones_like(drug_norms), drug_norms)

    # save train norms for reproducibility
    torch.save(cell_norms.detach().cpu(), f"train_cell_l2norms_run_{run_id}.pt")
    torch.save(drug_norms.detach().cpu(), f"train_drug_l2norms_run_{run_id}.pt")
    
    # RUS on training set
    rus = RandomUnderSampler(sampling_strategy="majority", random_state=RANDOM_SEED)
    X_train_bal, y_train_bal = rus.fit_resample(X_pairs.iloc[train_idx], y_all[train_idx])

    # split back into cell/drug + build val (unresampled)
    x_cell_train = X_train_bal.iloc[:, :n_cell]
    x_drug_train = X_train_bal.iloc[:, n_cell:]
    y_train = y_train_bal

    x_cell_val = X_pairs.iloc[val_idx, :n_cell]
    x_drug_val = X_pairs.iloc[val_idx, n_cell:]
    y_val = y_all[val_idx]

    print(f"x_cell_train shape (after RUS): {x_cell_train.shape}")
    print(f"x_drug_train shape (after RUS): {x_drug_train.shape}")
    print(f"x_cell_val shape:  {x_cell_val.shape}")
    print(f"x_drug_val shape:  {x_drug_val.shape}")
    print(f"y_train counts (after RUS): "
          f"0={np.sum(y_train_bal==0)}, 1={np.sum(y_train_bal==1)}")
    
    # Convert to PyTorch tensors
    x_cell_train_tensor = torch.Tensor(x_cell_train.values) / cell_norms
    x_drug_train_tensor = torch.Tensor(x_drug_train.values) / drug_norms
    x_cell_val_tensor = torch.Tensor(x_cell_val.values) / cell_norms
    x_drug_val_tensor = torch.Tensor(x_drug_val.values) / drug_norms
    
    y_train_tensor  = torch.Tensor(y_train).unsqueeze(1)
    y_val_tensor = torch.Tensor(y_val).unsqueeze(1)

    # Send to device
    x_cell_train_tensor = x_cell_train_tensor.to(device)
    x_drug_train_tensor = x_drug_train_tensor.to(device)
    y_train_tensor = y_train_tensor.to(device)
    x_cell_val_tensor = x_cell_val_tensor.to(device)
    x_drug_val_tensor = x_drug_val_tensor.to(device)
    y_val_tensor = y_val_tensor.to(device)

    # Create a TensorDataset with the input features and target labels
    train_dataset = TensorDataset(x_cell_train_tensor, x_drug_train_tensor, y_train_tensor)
    val_dataset = TensorDataset(x_cell_val_tensor, x_drug_val_tensor, y_val_tensor)

    # Create the train_loader and val_loader
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

    n_drug  = x_drug_train.shape[1]

    # Pretrained encoders
    encoder_cell = SimpleAutoencoder(x_cell_train_tensor.shape[1], cell_ae_latent_dim)
    encoder_cell.load_state_dict(torch.load("encoder_cell.pth"))
    
    encoder_drug = SimpleAutoencoder(x_drug_train_tensor.shape[1], drug_ae_latent_dim)
    encoder_drug.load_state_dict(torch.load("encoder_drug.pth"))
    
    model = STaRDR_pretrained(encoder_cell, encoder_drug, cell_ae_latent_dim, drug_ae_latent_dim, freeze_encoders=False).to(device)

    # Display model
    if run_id==0:
        print("\nModel architecture:\n")
        print(model)
    
    # Train the model
    train_mlp_with_encoders(model, train_loader, val_loader, num_epochs, run_id=run_id)

    # T-SNE (never / first / always)
    if visualize == 'always' or (visualize == 'first' and run_id == 0):
        with torch.no_grad():
            model.encoder_cell.eval(); model.encoder_drug.eval()
            z_cell = model.encoder_cell(x_cell_train_tensor)
            z_drug = model.encoder_drug(x_drug_train_tensor)
    
            def plot_tsne(z_tensor, y_tensor, title):
                z_embedded = TSNE(n_components=2, random_state=42).fit_transform(z_tensor.cpu().numpy())
                y_np = y_tensor.cpu().numpy().ravel()
                plt.figure(figsize=(8, 6))
                for label, color in zip([0, 1], ['blue', 'red']):
                    plt.scatter(z_embedded[y_np == label, 0], z_embedded[y_np == label, 1],
                                label='Resistant' if label == 0 else 'Sensitive',
                                c=color, s=10, alpha=0.7)
                plt.title(title)
                plt.legend()
                plt.tight_layout()
                plt.show()
    
            plot_tsne(z_cell, y_train_tensor, "t-SNE - z_cell")
            plot_tsne(z_drug, y_train_tensor, "t-SNE - z_drug")
            plot_tsne(torch.cat([z_cell, z_drug], dim=1), y_train_tensor, "t-SNE - z_cell + z_drug")

    return model, cell_norms.detach().cpu(), drug_norms.detach().cpu()

def train_mlp_with_encoders(model, train_loader, val_loader, num_epochs, run_id=0):
    """
    Supervised training loop for STaR-DR: finetune encoders + train MLP.
    
    - Saves trained modules:
        encoder_cell_trained_run_{run_id}.pt
        encoder_drug_trained_run_{run_id}.pt
        mlp_trained_run_{run_id}.pth

    Parameters:
    - model (STaRDR_pretrained): model containing encoders and MLP.
    - train_loader (DataLoader): training batches (post-RUS).
    - val_loader (DataLoader): validation batches (unresampled).
    - num_epochs (int): training epochs.
    - run_id (int): used to save trained modules.

    Return: model (STaRDR_pretrained): trained model.
    """
    
    # Optimizer: different learning rate (encoders vs. MLP)
    enc_lr = 1e-4
    mlp_lr = 5e-4
    optimizer = optim.Adam([
    {'params': model.encoder_cell.parameters(), 'lr': enc_lr, 'weight_decay': 1e-5},
    {'params': model.encoder_drug.parameters(), 'lr': enc_lr, 'weight_decay': 1e-5},
    {'params': model.mlp.parameters(),         'lr': mlp_lr, 'weight_decay': 1e-5},
])
    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=5, verbose=True)
    loss_fn = nn.BCELoss()

    train_losses = []
    val_losses = []
    train_accuracies = []
    val_accuracies = []

    for epoch in range(num_epochs):
        model.train()
        total_train_loss = 0.0
        train_preds = []
        train_targets = []
        for batch_idx, (cell_data, drug_data, target) in enumerate(train_loader):
            optimizer.zero_grad()
            y_pred, _, _ = model(cell_data, drug_data)
            loss = loss_fn(y_pred, target)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1)
            optimizer.step()
            total_train_loss += loss.item()
            
            # Collect predictions and targets for accuracy
            train_preds.extend((y_pred > 0.5).cpu().numpy())
            train_targets.extend(target.cpu().numpy())

        train_acc = np.mean(np.array(train_preds) == np.array(train_targets))
        train_accuracies.append(train_acc)

        # Validation
        model.eval()
        total_val_loss = 0.0
        val_preds = []
        val_targets = []
        
        with torch.no_grad():
            for val_batch_idx, (cell_data_val, drug_data_val, val_target) in enumerate(val_loader):
                y_val_pred, _, _ = model(cell_data_val, drug_data_val)
                val_loss = loss_fn(y_val_pred, val_target)
                total_val_loss += val_loss.item()

                val_preds.extend((y_val_pred > 0.5).cpu().numpy())
                val_targets.extend(val_target.cpu().numpy())

        val_acc = np.mean(np.array(val_preds) == np.array(val_targets))
        val_accuracies.append(val_acc)

        train_losses.append(total_train_loss / len(train_loader))
        val_losses.append(total_val_loss / len(val_loader))
        
        print(f"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}")
        
        scheduler.step(total_val_loss / len(val_loader))

    # Plot losses and accuracies
    Evaluation.plot_train_val_loss(train_losses, val_losses, num_epochs)
    Evaluation.plot_train_val_accuracy(train_accuracies, val_accuracies, num_epochs)

    # Save each models
    torch.save(model.encoder_cell, f"encoder_cell_trained_run_{run_id}.pt")
    torch.save(model.encoder_drug, f"encoder_drug_trained_run_{run_id}.pt")
    torch.save(model.mlp.state_dict(), f"mlp_trained_run_{run_id}.pth")
    
    return model

def test(model, test_loader, return_raw=False, show_plot=False):
    """
    Evaluate a trained model on a dataset loader.

    Collects predicted probabilities and true labels over all batches,
    then calls Evaluation.evaluate() to compute metrics.

    Parameters:
    - model (nn.Module): trained model.
    - test_loader (DataLoader): loader yielding (x_cell, x_drug, y).
    - return_raw (bool): if True, also returns y_true and y_score tensors.
    - show_plot (bool): passed to Evaluation.evaluate().

    Return:
    - result (dict): evaluation metrics.
    - (optional) y_true (Tensor), y_score (Tensor) if return_raw=True.
    """
    model.eval()
    
    preds_all = []
    labels_all = []    
    with torch.no_grad():
        for xc, xd, y in test_loader:
        # Forward pass through the model
            y_pred, _, _ = model(xc, xd)
            preds_all.append(y_pred.detach().cpu())
            labels_all.append(y.detach().cpu())

    # Concat all batches
    y_score = torch.cat(preds_all, dim=0)
    y_true = torch.cat(labels_all, dim=0)
    
    result = Evaluation.evaluate(y_true, y_score, show_plot=show_plot)

    if return_raw:
        return result, y_true, y_score
    return result

def cv_train(
    x_cell_train,
    x_drug_train, 
    y_train, 
    device, 
    k=5, 
    visualize='first', 
    run_id=0, 
    split='RANDOM'
):
    """
    Stratified K-fold cross-validation training for STaR-DR.

    For each fold:
    1) Split pairs into train/val folds (stratified).
    2) Compute normalization norms on the raw train fold (before RUS).
    3) Apply RUS on the train fold only.
    4) Normalize train (post-RUS) + val (unresampled) with the same norms.
    5) Load pretrained AEs, build STaR-DR model, train with train_mlp_with_encoders().
    6) Evaluate on the full validation fold and store metrics.

    Splitting modes: {'RANDOM','LCO','LDO'}
    The 'split' argument controls how folds are formed:

    - "RANDOM" (default):
        Standard StratifiedKFold on pairs (preserves class ratio per fold).

    - "LCO" (Leave-Cell-Out):
        Group-aware stratified CV where all pairs from the same cell are kept in the
        same fold (no cell leakage across train/val within a fold).

    - "LDO" (Leave-Drug-Out):
        Group-aware stratified CV where all pairs from the same drug are kept in the
        same fold (no drug leakage across train/val within a fold).

    Parameters:
    - x_cell_train, x_drug_train (pd.DataFrame): pair features.
    - y_train (array-like): labels.
    - device (torch.device): training device.
    - k (int): number of folds.
    - visualize (str): 'never' | 'first' | 'always' for t-SNE plots.
    - run_id (int): used for saving norms and weights.

    Return: history (dict[str, list[float]]): metrics aggregated across folds.
    """

    history = {'AUC': [], 'AUPRC': [], "Accuracy": [], 'Balanced Accuracy':[], "Precision": [], "Recall": [], "F1 score": []}
        
    # Concatenate to split/resample by pair
    X_pairs = pd.concat([x_cell_train, x_drug_train], axis=1)
    n_cell = x_cell_train.shape[1]
    
    # Labels as 0/1
    y_np = np.asarray(y_train).ravel().astype(int)

    # Compute groups if needed:
    groups = None
    # Group for LDO/LCO (LDO: by drug ID or LCO: by cell ID)
    if split.upper() == 'LDO':
        groups = parse_drug_ids_from_pairs(X_pairs.index)
    elif split.upper() == 'LCO':
        groups = parse_cell_ids_from_pairs(X_pairs.index)

    # Select splitter
    if split.upper() in ("LCO", "LDO"):
        cv = StratifiedGroupKFold(n_splits=k, shuffle=True, random_state=RANDOM_SEED)
        split_iter = cv.split(X_pairs, y_np, groups=groups)
    else:
        # StratifiedKFold preserves class proportions across folds
        cv = StratifiedKFold(n_splits=k, shuffle=True, random_state=RANDOM_SEED)
        split_iter = cv.split(np.zeros(len(y_np)), y_np)

    for fold, (train_data, val_data) in enumerate(split_iter):
        print(f"Fold {fold+1} ({split.upper()})")

        # split by pairs
        X_train, y_train = X_pairs.iloc[train_data], y_np[train_data]
        X_val, y_val = X_pairs.iloc[val_data],  y_np[val_data]

        # Normalize data
        thr = 1e-6
        x_cell_train_tensor = torch.tensor(X_train.iloc[:, :n_cell].values, dtype=torch.float32)
        x_drug_train_tensor = torch.tensor(X_train.iloc[:,  n_cell:].values, dtype=torch.float32)

        cell_norms = torch.norm(x_cell_train_tensor, dim=0, keepdim=True)
        cell_norms = torch.where(cell_norms < thr, torch.ones_like(cell_norms), cell_norms)

        drug_norms = torch.norm(x_drug_train_tensor, dim=0, keepdim=True)
        drug_norms = torch.where(drug_norms < thr, torch.ones_like(drug_norms), drug_norms)

        # save train norms for reproducibility
        torch.save(cell_norms.detach().cpu(), f"train_cell_l2norms_run_{run_id}.pt")
        torch.save(drug_norms.detach().cpu(), f"train_drug_l2norms_run_{run_id}.pt")

        # RUS on train fold
        rus = RandomUnderSampler(sampling_strategy="majority", random_state=RANDOM_SEED)
        X_train_bal, y_train_bal = rus.fit_resample(X_train, y_train)

        # Separate cells and drugs
        x_cell_train = X_train_bal.iloc[:, :n_cell]
        x_drug_train = X_train_bal.iloc[:, n_cell:]
        x_cell_val = X_val.iloc[:, :n_cell]
        x_drug_val = X_val.iloc[:, n_cell:]
        
        print(f"[Fold {fold+1}] x_cell_train shape (after RUS): {x_cell_train.shape}")
        print(f"[Fold {fold+1}] x_drug_train shape (after RUS): {x_drug_train.shape}")
        print(f"[Fold {fold+1}] x_cell_val shape:  {x_cell_val.shape}")
        print(f"[Fold {fold+1}] x_drug_val shape:  {x_drug_val.shape}")
        print(f"[Fold {fold+1}] y_train counts (after RUS): "
              f"0={np.sum(y_train_bal==0)}, 1={np.sum(y_train_bal==1)}")

        # Tensors
        x_cell_train_tensor = torch.Tensor(x_cell_train.values) / cell_norms
        x_drug_train_tensor = torch.Tensor(x_drug_train.values) / drug_norms
        x_cell_val_tensor = torch.Tensor(x_cell_val.values) / cell_norms
        x_drug_val_tensor = torch.Tensor(x_drug_val.values) / drug_norms
        y_train_tensor = torch.Tensor(y_train_bal).unsqueeze(1)
        y_val_tensor = torch.Tensor(y_val).unsqueeze(1)
        
        # Send to device
        x_cell_train_tensor = x_cell_train_tensor.to(device)
        x_drug_train_tensor = x_drug_train_tensor.to(device)
        y_train_tensor = y_train_tensor.to(device)
        x_cell_val_tensor = x_cell_val_tensor.to(device)
        x_drug_val_tensor = x_drug_val_tensor.to(device)
        y_val_tensor = y_val_tensor.to(device)
        
        # Create a TensorDataset with the input features and target labels
        train_dataset = TensorDataset(x_cell_train_tensor, x_drug_train_tensor, y_train_tensor)
        val_dataset   = TensorDataset(x_cell_val_tensor,  x_drug_val_tensor, y_val_tensor)
        
        # Create the train_loader and val_loader
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False)

        n_drug  = x_drug_train.shape[1]
        
        # Pretrained encoders
        encoder_cell = SimpleAutoencoder(n_cell, cell_ae_latent_dim)
        encoder_cell.load_state_dict(torch.load("encoder_cell.pth"))
        
        encoder_drug = SimpleAutoencoder(n_drug, drug_ae_latent_dim)
        encoder_drug.load_state_dict(torch.load("encoder_drug.pth"))

        # Model
        model = STaRDR_pretrained(encoder_cell, encoder_drug, cell_ae_latent_dim, drug_ae_latent_dim, freeze_encoders=False).to(device)

        # Display model
        if fold==0:
            print("\nModel architecture:\n")
            print(model)

        # Train model
        train_mlp_with_encoders(model, train_loader, val_loader, num_epochs, run_id=run_id)
      
        # Evaluate on the entire validation fold in a single pass
        val_loader_full = DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=False)
        results = test(model, val_loader_full)

        # Add results to the history dictionary
        Evaluation.add_results(history, results)

        # T-SNE (never / first / always)
        if visualize == 'always' or (visualize == 'first' and fold == 0):
            with torch.no_grad():
                model.encoder_cell.eval()
                model.encoder_drug.eval()
                z_cell_fold = model.encoder_cell(x_cell_train_tensor)
                z_drug_fold = model.encoder_drug(x_drug_train_tensor)
            
                def plot_tsne(z_tensor, y_tensor, title):
                    z_embedded = TSNE(n_components=2, random_state=42).fit_transform(z_tensor.detach().cpu().numpy())
                    y_np = y_tensor.detach().cpu().numpy().ravel()
                    plt.figure(figsize=(8, 6))
                    for label, color in zip([0, 1], ['blue', 'red']):
                        plt.scatter(z_embedded[y_np == label, 0], z_embedded[y_np == label, 1],
                                    label='Resistant' if label == 0 else 'Sensitive',
                                    c=color, s=10, alpha=0.7)
                    plt.title(title)
                    plt.legend()
                    plt.tight_layout()
                    plt.show()
        
                plot_tsne(z_cell_fold, y_train_tensor, "t-SNE - z_cell")
                plot_tsne(z_drug_fold, y_train_tensor, "t-SNE - z_drug")
                plot_tsne(torch.cat([z_cell_fold, z_drug_fold], dim=1), y_train_tensor, "t-SNE - z_cell + z_drug")

    return history

def run(k=10, is_test=False, visualize='first', split='RANDOM'):
    """
    Main entry point.

    - Loads training dataset (CTRP-GDSC).
    - Optionally loads an external test dataset (CCLE) and intersects features.
    - Saves feature_columns.pkl for reproducibility / downstream alignment.
    - Trains k independent runs (run_id = 0..k-1):
        is_test=True  -> 90/10 internal split training, then evaluate on external test.
        is_test=False -> cross-validation training on source (cv_train).

    Parameters:
    - k (int): number of independent runs.
    - is_test (bool): if True, evaluate on external dataset.
    - visualize (str): t-SNE policy: 'never' | 'first' | 'always'.

    Return: history (dict): aggregated metrics across runs.
    """
    
    # Initialization of metrics history
    history = {'AUC': [], 'AUPRC': [], "Accuracy": [], 'Balanced Accuracy':[], "Precision": [], "Recall": [], "F1 score": []}
    
    # Load training data
    train_data, train_drug_screen = RawDataLoader.load_data(
        data_modalities=DATA_MODALITIES,
        raw_file_directory=RAW_BOTH_DATA_FOLDER,
        screen_file_directory=BOTH_SCREENING_DATA_FOLDER,
        sep="\t"
    )

    print('train_data when loaded:', train_data.keys())
    for key, df in train_data.items():
        print(f"{key}: {df.shape}")
    
    # Load test data if applicable
    if is_test:
        test_data, test_drug_screen = RawDataLoader.load_data(
            data_modalities=DATA_MODALITIES,
            raw_file_directory=CCLE_RAW_DATA_FOLDER,  
            screen_file_directory=CCLE_SCREENING_DATA_FOLDER,
            sep="\t"
        )

        print('test_data when loaded:', test_data.keys())
        for key, df in test_data.items():
            print(f"{key}: {df.shape}")
                
        # Intersection of features between train and test
        train_data, test_data = RawDataLoader.data_features_intersect(train_data, test_data)

    # Save the feature columns for reproducibility:
    all_features = {}
    for key, df in train_data.items():
        all_features[key] = df.columns.tolist()
        
    with open("feature_columns.pkl", "wb") as f:
        pickle.dump(all_features, f)

    # Prepare input data for training
    x_cell_train, x_drug_train, y_train, cell_sizes, drug_sizes = RawDataLoader.prepare_input_data(train_data, train_drug_screen)

    if is_test:
        x_cell_test, x_drug_test, y_test, cell_sizes, drug_sizes = RawDataLoader.prepare_input_data(test_data, test_drug_screen)
        print(f"x_cell_test shape:  {x_cell_test.shape}")
        print(f"x_drug_test shape:  {x_drug_test.shape}")

    all_runs = []
    for i in range(k):
        print(f"\nRun {i+1}/{k}")
        run_id = i

        if is_test:
            
            # Train and evaluate the STaR-DR model on test data
            model, cell_norms, drug_norms = STaRDR_pretrained_training(
                x_cell_train, 
                x_drug_train, 
                y_train, 
                visualize=visualize, 
                run_id=i
            )

            # Convert test data to PyTorch tensors
            x_cell_test_tensor = torch.Tensor(x_cell_test.values).to(device)
            x_drug_test_tensor = torch.Tensor(x_drug_test.values).to(device)
            y_test_tensor = torch.Tensor(y_test).to(device)

        
            # normalize test set using train norms
            x_cell_test_tensor = x_cell_test_tensor / cell_norms
            x_drug_test_tensor = x_drug_test_tensor / drug_norms
          
            # Create a TensorDataset with the input features and target labels for testing
            test_dataset = TensorDataset(x_cell_test_tensor, x_drug_test_tensor, y_test_tensor)
            test_loader = DataLoader(test_dataset, batch_size=len(x_cell_test), shuffle=False)

            results, y_true, y_score = test(model, test_loader, return_raw=True,  show_plot=False)
                
            # Add the current run metrics to the history
            Evaluation.add_results(history, results)
            
            all_runs.append((y_true, y_score))

        else:

            # Train and evaluate the STaR-DR on the split data
            results = cv_train(
                x_cell_train, 
                x_drug_train, 
                y_train, 
                device, 
                k=5, 
                visualize='first', 
                run_id=i, 
                split=split
            )
            
            # cv_train returns lists (one per fold) -> extend history
            if isinstance(results.get('AUC', None), list):
                for m in history:
                    history[m].extend(results[m])
            else:
                Evaluation.add_results(history, results)

    # If external test mode: aggregate ROC/PR curves across runs
    if is_test and len(all_runs) > 0:
        fpr_grid, mean_tpr, std_tpr, auc_mean, auc_std = Evaluation.aggregate_roc(all_runs, n_points=200)
        rec_grid, mean_prec, std_prec, auprc_mean, auprc_std = Evaluation.aggregate_pr(all_runs, n_points=200)
        Evaluation.plot_mean_roc(fpr_grid, mean_tpr, std_tpr, auc_mean, auc_std, label="STaR-DR")
        Evaluation.plot_mean_pr(rec_grid, mean_prec, std_prec, auprc_mean, auprc_std, label="STaR-DR")
        
    # Display final results
    Evaluation.show_final_results(history)
    return history

# -----------------------------
# Script execution
# -----------------------------
# Choose number of runs, execution mode and split strategy.
#  
#    Number of runs:
#    - The first argument of run(k) controls the number of independent runs.
#    - Each run uses a different random split.
#    - When is_test = False, each run performs a full k-fold cross-validation.
#    - When is_test = True, each run trains once and evaluates on the same external test set.
#
#    Execution mode:
#    - is_test = False   # Cross-validation on the training dataset (internal evaluation)
#    - is_test = True    # Train on training dataset and evaluate once on an external test dataset
#    
#    Split strategy (controls how train/validation splits are built):
#    - split = "RANDOM"  # standard stratified pair split
#    - split = "LCO"     # leave-cell-out 
#    - split = "LDO"     # leave-drug-out 
#
#    NOTE:
#    In the current implementation, Leave-Out protocols (LCO / LDO) are
#    applied ONLY in 'cv_train'. Therefore, when 'is_test = True', the 'split'
#    argument is ignored and the internal train/validation split performed
#    inside 'STaRDR_pretrained_training' always corresponds to a standard stratified
#    pair-level split ("RANDOM").
if __name__ == "__main__":
    # Ensure reproducibility for python / numpy / torch RNGs
    torch.manual_seed(RANDOM_SEED)
    random.seed(RANDOM_SEED)
    np.random.seed(RANDOM_SEED)
    
    run(k=1, is_test=False, split='RANDOM')